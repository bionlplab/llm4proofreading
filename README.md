# Reportedly LLMs: Generative Large Language Models for Proofreading Errors in Radiology Reports

This is the repository for Reportedly LLMs, which aims to build task-specific LLMs for medical proofreading. 


## Overview

The overall workflow of Reportedly LLMs.

Figure 1 will be presented here after the manuscript is published.

Our work consists of three parts:

(1). Dataset Construction

(2). Model Development

(3). Medical Proofreading

## Dataset Construction

We constructed a dataset consisting of two parts. 

The first part includes 1,656 synthetic radiology reports generated by GPT-4 using specified prompts, divided into 828 error-free synthetic reports and 828 synthetic reports with errors. 

```
Reffers to Prompts_for_Synthetic.txt
```

The second part comprises 614 reports: 307 errorfree reports from the MIMIC-CXR database, and 307 corresponding synthetic reports with errors generated by GPT-4 based on these MIMIC-CXR reports and specified prompts.

```
Reffers to Prompts_for_MIMIC.txt
```

## Model Development

We fine-tune our models using standard Hugging Face training code.
We fine-tune Llama-3-8B-Instruct and Llama-3-70B-Instruct with the following hyperparameters:

| Hyperparameter    | Llama-3-8B-Instruct | Llama-3-70B-Instruct |
|-------------------|---------------------|----------------------|
| Batch size        | 1                   | 1                    |
| Learning rate     | 3e-4                | 3e-4                 |
| Epochs            | 3                   | 3                    |
| Max length        | 512                 | 512                  |


To reproduce our fine-tuning runs for LLaMA, first install the requirements

```bash
pip install -r requirements.txt
```

Below is a command that fine-tunes Llama-3-70B-Instruct with our training set on a server with 2 A100 80G GPUs.

We were able to reproduce a model of similar quality as the one we hosted in our demo with the following command using **Python 3.10**.

Replace `<your_random_port>` with a port of your own and `<your_output_dir>` with where you want to store your outputs.

```bash
torchrun --nproc_per_node=4 --master_port=<your_random_port> train.py \
    --model_name_or_path <your_path_to_hf_converted_llama_ckpt_and_tokenizer> \
    --data_path ./alpaca_data.json \
    --bf16 True \
    --output_dir <your_output_dir> \
    --num_train_epochs 3 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 4 \
    --gradient_accumulation_steps 8 \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps 2000 \
    --save_total_limit 1 \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --fsdp "full_shard auto_wrap" \
    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \
    --tf32 True
```


The same script also works for Llama-3-8B-Instruct fine-tuning. Here's an example for fine-tuning Llama-3-8B-Instruct.

```bash
torchrun --nproc_per_node=4 --master_port=<your_random_port> train.py \
    --model_name_or_path "facebook/opt-6.7b" \
    --data_path ./alpaca_data.json \
    --bf16 True \
    --output_dir <your_output_dir> \
    --num_train_epochs 3 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 4 \
    --gradient_accumulation_steps 8 \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps 2000 \
    --save_total_limit 1 \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --fsdp "full_shard auto_wrap" \
    --fsdp_transformer_layer_cls_to_wrap 'OPTDecoderLayer' \
    --tf32 True
```

## Medical Proofreading

XXX

### Authors

XXX

### Citation

Please cite the repo if you use the data or code in this repo.

```
@misc{alpaca,
  author = {XXX },
  title = {Reportedly LLMs: Generative Large Language Models for Proofreading Errors in Radiology Reports},
  year = {2024},
  publisher = {XXX},
  journal = {XXX},
}
```

### Acknowledgements

XXX
